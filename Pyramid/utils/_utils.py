import os
import anndata
import numpy as np
import pandas as pd
import scanpy as sc
import scipy
import random

from sklearn.preprocessing import OneHotEncoder

from typing import TypeVar
A = TypeVar('anndata')  ## generic for anndata
ENC = TypeVar('OneHotEncoder')

from models.distiller import MLP, Distiller

Ftest_Rcode = "Rcode/Ftest_selection.R"
RANDOM_SEED = 1993
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
Teacher_DIMS = [128, 32, 8]
Student_DIMS = [64, 16]
Celltype_COLUMN = "celltype"
PredCelltype_COLUMN = "pred_celltype"

def _COOmtx_data_loader(mtx_prefix: str) -> A:
    '''
    Load gene score matrix in COO format
    ---
    Input:
        - mtx_prefix: gene score matrix in COO format
    ---
    Output:
        - an anndata object
    '''
    adata = anndata.read_mtx(mtx_prefix+'.mtx.gz').T
    genes = pd.read_csv(mtx_prefix+'_genes.tsv', header=None, sep='\t')
    adata.var["genes"] = genes[0].values
    adata.var_names = adata.var["genes"]
    adata.var_names_make_unique(join="-")
    adata.var.index.name = None
    cells = pd.read_csv(mtx_prefix+'_barcodes.tsv', header=None, sep='\t')
    adata.obs["barcode"] = cells[0].values
    adata.obs_names = adata.obs['barcode']
    adata.obs_names_make_unique(join="-")
    adata.obs.index.name = None

    adata = adata[:, adata.var_names.notnull()]
    adata.var_names=[i.upper() for i in list(adata.var_names)]
    return adata

def _csv_data_loader(csv_input: str) -> A:
    '''
    Load gene score matrix in csv format
    ---
    Input:
        - csv_input: gene scre matrix in dense format with row as genes and cells as columns.
    ---
    Output:
        - an anndata object
    '''
    df = pd.read_csv(csv_input, index_col=0)
    obs = pd.DataFrame(data=df.columns, index=df.columns)
    obs.columns = ["barcode"]
    var = pd.DataFrame(data=df.index, index=df.index)
    var.columns = ['gene_symbols']
    adata = anndata.AnnData(X=df.T, obs=obs, var=var)
    adata.obs_names_make_unique(join="-")
    adata.var_names_make_unique(join="-")

    adata = adata[:, adata.var_names.notnull()]
    adata.var_names=[i.upper() for i in list(adata.var_names)]
    return adata

def _metadata_loader(metadata):
    '''Load metadata
    '''
    metadata = pd.read_csv(metadata, index_col=0, sep=',')
    return metadata


def _process_adata(adata, celltype_label='celltype'):
    '''Procedures for filtering single-cell gene scale data (can be gene expression, or gene scores)
       1. Filter nonsense genes;
       2. Normalize and log-transform the data;
       3. Remove cells with no labels; 
    '''
    adata = adata[:, adata.var_names.notnull()]  ## remove NA var_names, some genes generated by ArchR gene scores will be NA
    adata.var_names=[i.upper() for i in list(adata.var_names)] #avoid some genes having lower letter

    ## make names unique after removing
    adata.var_names_make_unique()
    adata.obs_names_make_unique()

    #prefilter_specialgene: MT and ERCC  -> refered from ItClust package
    Gene1Pattern="ERCC"
    Gene2Pattern="MT-"
    id_tmp1=np.asarray([not str(name).startswith(Gene1Pattern) for name in adata.var_names],dtype=bool)
    id_tmp2=np.asarray([not str(name).startswith(Gene2Pattern) for name in adata.var_names],dtype=bool)
    id_tmp=np.logical_and(id_tmp1,id_tmp2)
    adata._inplace_subset_var(id_tmp)

    ## handel exception when there are not enough cells or genes after filtering
    if adata.shape[0] < 3 or adata.shape[1] < 3:
        sys.exit("Error: too few genes or cells to continue..")

    ## normalization,var.genes,log1p
    sc.pp.normalize_per_cell(adata, min_counts=0)
    sc.pp.log1p(adata)

    ## cells with celltypes
    cells = adata.obs.dropna(subset=[celltype_label]).index.tolist()
    adata = adata[cells]
    return adata

def _select_feature(adata: A, tmp_dir=None, fs_method = "F-test", num_features: int = 1000) -> A:
    '''Select features
    ---
    Input:
        - anndata
        - tmp_dir: temporary dir for F-test
        - fs_method: F-test / noFS / seurat
    '''
    if fs_method == "F-test":
        print("Use F-test to select features.\n")
        if scipy.sparse.issparse(adata.X) or \
                isinstance(adata.X, pd.DataFrame):
            tmp_data = adata.X.toarray()
        else:
            tmp_data = adata.X
        ## write out original read count matrix
        tmp_df = pd.DataFrame(data=np.round(tmp_data, 3), 
                index=adata.obs_names, columns=adata.var_names).T
        tmp_df_path = tmp_dir+os.sep+"tmp_counts.csv"
        tmp_df.to_csv(tmp_df_path)
        ## write out cell annotations based on train
        cell_annots = adata.obs[Celltype_COLUMN].tolist()
        cell_annots_path = tmp_dir+os.sep+"tmp_cell_annots.txt"
        with open(cell_annots_path, 'w') as f:
            for cell_annot in cell_annots:
                f.write("%s\n" % cell_annot)
        os.system("Rscript --vanilla " + Ftest_Rcode + " "+ tmp_df_path + " " + 
                cell_annots_path + " " + str(num_features))
        os.system("rm {}".format(cell_annots_path))  ## remove the temporaty cell annotations
        os.system("rm {}".format(tmp_df_path))  ## remove the temporaty counts

        ftest_file = tmp_dir+os.sep+'F-test_features.txt'
        with open(ftest_file) as f:
            features = f.read().splitlines()
        features.sort()
        train_adata = train_adata[:, features]

    if fs_method == "seurat":
        print("Use seurat in scanpy to select features.\n")
        sc.pp.highly_variable_genes(adata, n_top_genes=num_features, subset=True)
        seurat_file = tmp_dir+os.sep+'Seurat_features.txt'
        with open(seurat_file, 'w') as f:
            f.writelines(adata.var_names.tolist())

    return adata


def _scale_data(adata):
    '''Center scale
    '''
    sc.pp.scale(adata, zero_center=True, max_value=6)
    return adata

def _visualize_data(adata, output_dir, color_columns=["celltype"],
        reduction="tSNE", prefix="data"):
    '''Visualize data 

    ---
    Input:
        - reduction: tSNE or UMAP
        - color_columns: plot on categories
    '''
    sc.tl.pca(adata, random_state=RANDOM_SEED)

    if reduction == "tSNE":
        sc.tl.tsne(adata, use_rep="X_pca",
            learning_rate=300, perplexity=30, n_jobs=4, random_state=dr_seed)
        sc.pl.tsne(adata, color=color_columns)
        plt.tight_layout()
        plt.savefig(output_dir+os.sep+prefix+"tSNE_cluster.png")
    if reduction == "UMAP":
        sc.pp.neighbors(adata, n_neighbors=20, use_rep="X_pca", random_state=RANDOM_SEED) 
        sc.tl.umap(adata, random_state=RANDOM_SEED)
        sc.pl.umap(adata, color=color_columns)
        plt.tight_layout()
        plt.savefig(output_dir+os.sep+prefix+"umap_cluster.png")

def _save_adata(adata, output_dir, prefix=""):
    '''Save anndata as h5ad
    '''
    adata.write(output_dir+os.sep+prefix+'adata.h5ad')


def _prob_to_label(enc: ENC, y_pred: np.ndarray) -> np.ndarray:
    '''Turn predicted probabilites to labels
    --- 
    Input:
        - enc: Encoder object
        - y_pred: Predicted probabilities
    ---
    Output:
        - an numpy ndarray containing predicted cell types
    '''
    pred_labels = y_pred.argmax(1)
    n_clusters = len(enc.categories_[0])  ## number of cell types from enc
    pred_onehot = np.zeros((pred_labels.size, n_clusters))
    pred_onehot[np.arange(pred_labels.size), pred_labels] = 1
    pred_celltypes = enc.inverse_transform(pred_onehot)
    print("=== Predicted celltypes: ", set(pred_celltypes.flatten()))
    return pred_celltypes.flatten() ## (N,)

def _extract_adata(adata: A) -> np.ndarray:
    '''Extract adata.X to a numpy array
    ---
    Output:
         - matrix in np.ndarray format
    '''
    if scipy.sparse.issparse(adata.X):
        X = adata.X.toarray()
    else:
        X = adata.X
    return X

def _init_MLP(x_train, y_train, dims=[64, 16], seed=0):
    '''Initialize MLP model based on input data
    '''
    mlp = MLP(dims)
    mlp.input_shape = (x_train.shape[1], )
    mlp.n_classes = len(set(y_train.argmax(1)))
    mlp.random_state = seed
    mlp.init_MLP_model()  ## init the model
    return mlp


def _identify_low_entropy_cells(test_adata, low_entropy_quantile,
        pred_celltype_cols):
    '''Select low entropy cells from each cell type
    ---
    Output:
        - test anndata: with 'entropy_status' added
    '''
    low_entropy_cells = []
    for celltype in set(test_adata.obs[pred_celltype_cols]):
        celltype_df = test_adata.obs[test_adata.obs[pred_celltype_cols] == celltype]
        entropy_cutoff = np.quantile(celltype_df['entropy'], q=low_entropy_quantile)
        cells = celltype_df.index[np.where(celltype_df['entropy'] < entropy_cutoff)[0]].tolist()
        low_entropy_cells.extend(cells)
    high_entropy_cells = list(set(test_adata.obs_names) - set(low_entropy_cells))
    test_adata.obs.loc[low_entropy_cells, 'entropy_status'] = "low"
    test_adata.obs.loc[high_entropy_cells, 'entropy_status'] = "high"
    return test_adata

def run_MLP(x_train, y_train, x_test, dims=[64, 16],
        batch_size=128, seed=0, save_dir=None,
        sample_weight=None, class_weight=None):
    '''Pipeline for MLP
    '''
    mlp = _init_MLP(x_train, y_train, dims, seed)
    mlp.compile()

    if save_dir is None:
        mlp.fit(x_train, y_train, batch_size=batch_size,
                sample_weight=sample_weight, class_weight=class_weight)
    else:
        if os.path.exists(save_dir):
            ## load weights from saved checkpoint
            mlp.model.load_weights(save_dir)
        else:
            mlp.fit(x_train, y_train, batch_size=batch_size,
                    sample_weight=sample_weight, class_weight=class_weight)
            mlp.model.save_weights(save_dir)
    mlp.model.summary()

    ## only calculate predicting time
    start = time.time()
    y_pred = mlp.predict(x_test)
    end = time.time()
    return y_pred, end-start

def run_distiller(x_train, y_train, student_model, teacher_model,
        epochs=30, alpha=0.1, temperature=3):
    '''Pipeline for Distiller
    '''
    distiller = Distiller(student=student_model, teacher=teacher_model)
    distiller.compile(
        optimizer=tf.keras.optimizers.Adam(),
        metrics=["accuracy"],
        student_loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
        distillation_loss_fn=tf.keras.losses.KLDivergence(),
        alpha=alpha,
        temperature=temperature,
    )
    distiller.fit(x_train, y_train, epochs=epochs,
            validation_split=0.0, verbose=2)
    return distiller
 
