import os
import anndata
import numpy as np
import scipy

from sklearn.preprocessing import OneHotEncoder

from typing import TypeVar
A = TypeVar('anndata')  ## generic for anndata
ENC = TypeVar('OneHotEncoder')

from ..models.distiller import MLP, Distiller
Ftest_Rcode = "Rcode/Ftest_selection.R"

def _COOmtx_data_loader(mtx_prefix: str) -> A:
    '''
    Load gene score matrix in COO format
    ---
    Input:
        - mtx_prefix: gene score matrix in COO format
    ---
    Output:
        - an anndata object
    '''
    adata = anndata.read_mtx(mtx_prefix+'_genescore.mtx.gz').T
    genes = pd.read_csv(mtx_prefix+'_genes.tsv', header=None, sep='\t')
    adata.var["genes"] = genes[0].values
    adata.var_names = adata.var["genes"]
    adata.var_names_make_unique(join="-")
    adata.var.index.name = None
    cells = pd.read_csv(mtx_prefix++'_cells.tsv', header=None, sep='\t')
    adata.obs["barcode"] = cells[0].values
    adata.obs_names = adata.obs['barcode']
    adata.obs_names_make_unique(join="-")
    adata.obs.index.name = None

    adata = adata[:, adata.var_names.notnull()]
    adata.var_names=[i.upper() for i in list(adata.var_names)]
    return adata

def _csv_data_loader(csv_input: str) -> A:
    '''
    Load gene score matrix in csv format
    ---
    Input:
        - csv_input: gene scre matrix in dense format with row as genes and cells as columns.
    ---
    Output:
        - an anndata object
    '''
    df = pd.read_csv(csv_input, index_col=0)
    obs = pd.DataFrame(data=df.columns, index=df.columns)
    obs.columns = ["barcode"]
    var = pd.DataFrame(data=df.index, index=df.index)
    var.columns = ['gene_symbols']
    adata = anndata.AnnData(X=df.T, obs=obs, var=var)
    adata.obs_names_make_unique(join="-")
    adata.var_names_make_unique(join="-")

    adata = adata[:, adata.var_names.notnull()]
    adata.var_names=[i.upper() for i in list(adata.var_names)]
    return adata

def _metadata_loader(metadata):
    '''Load metadata
    '''
    metadata = pd.read_csv(metadata, index_col=0, sep=',')
    return metadata


def _process_adata(adata, celltype_label='celltype'):
    '''Procedures for filtering single-cell gene scale data (can be gene expression, or gene scores)
       1. Filter nonsense genes;
       2. Normalize and log-transform the data;
       3. Remove cells with no labels; 
    '''
    adata = adata[:, adata.var_names.notnull()]  ## remove NA var_names, some genes generated by ArchR gene scores will be NA
    adata.var_names=[i.upper() for i in list(adata.var_names)] #avoid some genes having lower letter

    ## make names unique after removing
    adata.var_names_make_unique()
    adata.obs_names_make_unique()

    #prefilter_specialgene: MT and ERCC  -> refered from ItClust package
    Gene1Pattern="ERCC"
    Gene2Pattern="MT-"
    id_tmp1=np.asarray([not str(name).startswith(Gene1Pattern) for name in adata.var_names],dtype=bool)
    id_tmp2=np.asarray([not str(name).startswith(Gene2Pattern) for name in adata.var_names],dtype=bool)
    id_tmp=np.logical_and(id_tmp1,id_tmp2)
    adata._inplace_subset_var(id_tmp)

    ## handel exception when there are not enough cells or genes after filtering
    if adata.shape[0] < 3 or adata.shape[1] < 3:
        sys.exit("Error: too few genes or cells to continue..")

    ## normalization,var.genes,log1p
    sc.pp.normalize_per_cell(adata, min_counts=0)
    sc.pp.log1p(adata)

    ## cells with celltypes
    cells = adata.obs.dropna(subset=[celltype_label]).index.tolist()
    adata = adata[cells]
    return adata


def _prob_to_label(enc: ENC, y_pred: np.ndarray) -> np.ndarray:
    '''Turn predicted probabilites to labels
    --- 
    Input:
        - enc: Encoder object
        - y_pred: Predicted probabilities
    ---
    Output:
        - an numpy ndarray containing predicted cell types
    '''
    pred_labels = y_pred.argmax(1)
    n_clusters = len(enc.categories_[0])  ## number of cell types from enc
    pred_onehot = np.zeros((pred_labels.size, n_clusters))
    pred_onehot[np.arange(pred_labels.size), pred_labels] = 1
    pred_celltypes = enc.inverse_transform(pred_onehot)
    print("=== Predicted celltypes: ", set(pred_celltypes.flatten()))
    return pred_celltypes.flatten() ## (N,)

def _prepare_data(train_adata: A, test_adata: A, 
        enc: ENC = None, celltype_cols: str = "cell.type"):
    '''Prepare training and test anndata
    ---
    Output:
        - encoder: new encoder
        - x_train, y_train, x_test
    '''
    if scipy.sparse.issparse(train_adata.X):
        x_train = train_adata.X.toarray()
    else:
        x_train = train_adata.X

    if scipy.sparse.issparse(test_adata.X):
        x_test = test_adata.X.toarray()
    else:
        x_test = test_adata.X
 
    if enc is None:
        enc = OneHotEncoder(handle_unknown='ignore')
        y_train = enc.fit_transform(train_adata.obs[[celltype_cols]]).toarray()
    else:
        y_train = enc.transform(train_adata.obs[[celltype_cols]]).toarray()
    return enc, x_train, y_train, x_test

def _identify_low_entropy_cells(test_adata, low_entropy_quantile,
        pred_celltype_cols):
    '''Select low entropy cells from each cell type
    ---
    Output:
        - test anndata: with 'entropy_status' added
    '''
    low_entropy_cells = []
    for celltype in set(test_adata.obs[pred_celltype_cols]):
        celltype_df = test_adata.obs[test_adata.obs[pred_celltype_cols] == celltype]
        entropy_cutoff = np.quantile(celltype_df['entropy'], q=low_entropy_quantile)
        cells = celltype_df.index[np.where(celltype_df['entropy'] < entropy_cutoff)[0]].tolist()
        low_entropy_cells.extend(cells)
    high_entropy_cells = list(set(test_adata.obs_names) - set(low_entropy_cells))
    test_adata.obs.loc[low_entropy_cells, 'entropy_status'] = "low"
    test_adata.obs.loc[high_entropy_cells, 'entropy_status'] = "high"
    return test_adata


def _init_MLP(x_train, y_train, dims=[64, 16], seed=0):
    '''Initialize MLP model based on input data
    '''
    mlp = MLP(dims)
    mlp.input_shape = (x_train.shape[1], )
    mlp.n_classes = len(set(y_train.argmax(1)))
    mlp.random_state = seed
    mlp.init_MLP_model()  ## init the model
    return mlp

def run_MLP(x_train, y_train, x_test, dims=[64, 16],
        batch_size=128, seed=0, save_dir=None,
        sample_weight=None, class_weight=None):
    '''Pipeline for MLP
    '''
    mlp = _init_MLP(x_train, y_train, dims, seed)
    mlp.compile()

    if save_dir is None:
        mlp.fit(x_train, y_train, batch_size=batch_size,
                sample_weight=sample_weight, class_weight=class_weight)
    else:
        if os.path.exists(save_dir):
            ## load weights from saved checkpoint
            mlp.model.load_weights(save_dir)
        else:
            mlp.fit(x_train, y_train, batch_size=batch_size,
                    sample_weight=sample_weight, class_weight=class_weight)
            mlp.model.save_weights(save_dir)
    mlp.model.summary()

    ## only calculate predicting time
    start = time.time()
    y_pred = mlp.predict(x_test)
    end = time.time()
    return y_pred, end-start

def run_distiller(x_train, y_train, student_model, teacher_model,
        epochs=30, alpha=0.1, temperature=3):
    '''Pipeline for Distiller
    '''
    distiller = Distiller(student=student_model, teacher=teacher_model)
    distiller.compile(
        optimizer=tf.keras.optimizers.Adam(),
        metrics=["accuracy"],
        student_loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
        distillation_loss_fn=tf.keras.losses.KLDivergence(),
        alpha=alpha,
        temperature=temperature,
    )
    distiller.fit(x_train, y_train, epochs=epochs,
            validation_split=0.0, verbose=2)
    return distiller
 
