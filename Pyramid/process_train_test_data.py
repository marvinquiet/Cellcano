import os, sys
import random
import numpy as np
import pandas as pd
import scipy
from scipy.sparse import csr_matrix
import scanpy.api as sc
import anndata

## plot
import matplotlib.pyplot as plt

FEAST_SC3_RSCRIPT_PATH = "/homelocal/wma36/celltyping_refConstruct/pipelines/Rcode/FEAST_selection.R"
FEAST_FTEST_RSCRIPT_PATH = "/homelocal/wma36/celltyping_refConstruct/pipelines/Rcode/Ftest_selection.R"

## ---- some functions for processing data
def process_adata(adata, min_genes=10, min_cells=10, celltype_label="cell.type",
        preprocess=True, log=True):
    '''Procedures for filtering single-cell gene scale data (can be gene expression, or gene scores)
       1. Filter low-quality cells and genes;
       2. Filter nonsense genes;
       3. Normalize and log-transform the data;
       4. Change all gene names into UPPER;
       5. Remove cells with no labels; 
    '''
    adata = adata[:, adata.var_names.notnull()]  ## remove NA var_names, some genes generated by ArchR gene scores will be NA
    adata.var_names=[i.upper() for i in list(adata.var_names)] #avoid some genes having lower letter

    ## make names unique
    adata.var_names_make_unique()
    adata.obs_names_make_unique()

    if preprocess: 
        #1.pre filter cells
        # prefilter_cells(adata,min_genes=min_genes) 
        sc.pp.filter_cells(adata, min_genes=min_genes)

        #2 pre_filter genes
        #prefilter_genes(adata,min_cells=min_cells) # avoiding all gene is zeros
        sc.pp.filter_genes(adata, min_cells=min_cells)

    #3 prefilter_specialgene: MT and ERCC  -> from ItClust package
    Gene1Pattern="ERCC"
    Gene2Pattern="MT-"
    id_tmp1=np.asarray([not str(name).startswith(Gene1Pattern) for name in adata.var_names],dtype=bool)
    id_tmp2=np.asarray([not str(name).startswith(Gene2Pattern) for name in adata.var_names],dtype=bool)
    id_tmp=np.logical_and(id_tmp1,id_tmp2)
    adata._inplace_subset_var(id_tmp)

    ## handel exception when there are not enough cells or genes after filtering
    if adata.shape[0] < 3 or adata.shape[1] < 3:
        return None

    #4 normalization,var.genes,log1p
    #if preprocess: 
    sc.pp.normalize_per_cell(adata, min_counts=0)
    ## normalize by total counts, without filtering out cells
    
    ## total count equal to the median of the counts_per_cell, not applicable for gene scores
    if log:  ## whether to log data to get features from Seurat
        sc.pp.log1p(adata)

    ## cells with celltypes
    cells = adata.obs.dropna(subset=[celltype_label]).index.tolist()
    adata = adata[cells]
    return adata

def process_ATACseq_adata(adata, min_regions=0, celltype_label="cell.type",
        preprocess=True):
    '''Process scATACseq input, ATAC bins are binarized and then TF-IDF transformation

    We use all regions as input
    '''
    ## make names unique
    adata.obs_names_make_unique()

    if preprocess: 
        #1.pre filter cells but remain all regions
        sc.pp.filter_cells(adata, min_genes=min_regions)

    ## handel exception when there are not enough cells or regions after filtering
    if adata.shape[0] < 3 or adata.shape[1] < 3:
        return None

    # TF-IDF, log(TF*IDF*10^4 + 1) from Stuart & Butler et al. 2019
    ## TF-IDF, TF * log(IDF) from Cusanovich & Hill 2018 ## this one results in a lot of zeros
    nregions = np.asarray(adata.X.sum(axis=1)).reshape(-1)
    tf = np.matmul(adata.T.X.toarray(), np.diag(1/nregions)) ## number of regions * number of cells
    ncells = np.asarray(adata.X.sum(axis=0)).reshape(-1)
    idf = adata.shape[0]/ncells ## number of regions * 1
    idf[idf == np.inf] = 0  ## replace with 0
    #idf = np.log(1+idf)
    #norm.data = np.matmul(np.diag(idf), tf)  ## this was too large for the diagonal matrix
    norm_data = np.zeros((adata.shape[1], adata.shape[0]))
    for i in range(adata.shape[1]):
        norm_data[i,] = np.dot(idf[i], tf[i,])
    tfidf_res = np.log(1+norm_data*1e4)
    tfidf_res = np.around(tfidf_res, decimals=3)
    adata.X = csr_matrix(tfidf_res.T)

    #if SVD: ## SVD of Cusanovish, we currently remove this part
    #    ## do SVD and remove the first component
    #    n = 50 ## reduced dimension
    #    import scipy.sparse.linalg
    #    u, s, v = scipy.sparse.linalg.svds(adata.X.toarray(), k=n)
    #    norm_data = (u-np.mean(u, axis=0))/np.std(u, axis=0)
    #    norm_data = norm_data[:, 1:] ## remove the first dimension
    #    svd_adata = anndata.AnnData(X=norm_data, obs=adata.obs)
    #    adata = svd_adata.copy()

    ##3 normalization,var.genes,log1p
    #if preprocess: 
    #    sc.pp.normalize_per_cell(adata)  ## total count equal to the median of the counts_per_cell, not applicable for gene scores
    #if log:  ## whether to log data to get features from Seurat
    #    sc.pp.log1p(adata)

    ## cells with celltypes
    cells = adata.obs.dropna(subset=[celltype_label]).index.tolist()
    adata = adata[cells]
    return adata


def feature_selection_train_test(train_adata, test_adata, result_dir,
        gene_no=1000, select_on="test", select_method="Seurat",
        min_genes=0, min_cells=0, celltype_label="cell.type",
        preprocess=True, log=True):
    '''Perform feature selection on train_adata and test_adata

    @ result_dir: when method is FEAST, for storing temp counts and FEAST features
    @ gene_no: top x numbers of features
    @ select_on: whether perform on test or train
    @ select_method: Seurat/FEAST/F-test
        FEAST(unsupervised)/F-test(supervised) based on FEAST implementation and
        can be only applied to training datasets because we do not know labels
        for test
    @ min_genes/min_cells: when processing the data, the minimun requirements
    @ celltype_label: which column indicates the cell type
    @ preprocess: whether to pre-process the data such as filtering, normalize and log1p
    '''

    ## if feature already exists
    feature_file = result_dir+os.sep+"features.txt"
    if os.path.exists(feature_file):
        ## filter cells/genes, etc
        train_adata = process_adata(train_adata, min_genes, min_cells, preprocess=preprocess, log=log)
        test_adata = process_adata(test_adata, min_genes, min_cells, preprocess=preprocess, log=log)

        with open(feature_file) as f:
            features = f.read().splitlines()
        print("Number of features:", len(features))

        features = set(train_adata.var_names.tolist()).intersection(set(features))
        features = set(test_adata.var_names.tolist()).intersection(set(features))
        features = list(features)
        features.sort()  ## for reproducibility

        ## order common genes in anndata
        train_adata = train_adata[:, features]
        test_adata = test_adata[:, features]
        return train_adata, test_adata

    if "FEAST" == select_method or "F-test" == select_method:
        ## write file to result dir and run Rscript
        if select_on == "test":
            tmp_adata = test_adata.copy()
        if select_on == "train":
            tmp_adata = train_adata.copy()
        ## to csv
        tmp_data = None
        if scipy.sparse.issparse(tmp_adata.X) or \
                isinstance(tmp_adata.X, pd.DataFrame):
            tmp_data = tmp_adata.X.toarray()
        else:
            tmp_data = tmp_adata.X

        ## write out original read count matrix
        tmp_df = pd.DataFrame(data=tmp_data, index=tmp_adata.obs_names, columns=tmp_adata.var_names).T
        tmp_df_path = result_dir+os.sep+"tmp_counts.csv"
        tmp_df.to_csv(tmp_df_path)

        if "FEAST" == select_method:
            os.system("Rscript --vanilla " + FEAST_SC3_RSCRIPT_PATH + " "+ tmp_df_path + " " + 
                    str(len(set(tmp_adata.obs[celltype_label]))) + " " + str(gene_no))
        elif "F-test" == select_method and "train" == select_on:
            ## write out cell annotations based on train
            cell_annots = tmp_adata.obs[celltype_label].tolist()
            cell_annots_path = result_dir+os.sep+"tmp_cell_annots.txt"
            with open(cell_annots_path, 'w') as f:
                for cell_annot in cell_annots:
                    f.write("%s\n" % cell_annot)
            os.system("Rscript --vanilla " + FEAST_FTEST_RSCRIPT_PATH + " "+ tmp_df_path + " " + 
                    cell_annots_path + " " + str(gene_no))
            os.system("rm {}".format(cell_annots_path))  ## remove the temporaty cell annotations

        os.system("rm {}".format(tmp_df_path))  ## remove the temporaty counts
        del tmp_adata

    ## filter cells/genes, etc
    train_adata = process_adata(train_adata, min_genes, min_cells, preprocess=preprocess, log=log)
    test_adata = process_adata(test_adata, min_genes, min_cells, preprocess=preprocess, log=log)

    ## handle None exception
    if train_adata is None or test_adata is None:
        return None, None

    ## select top 1000 HVGs from test
    if select_method == "Seurat":
        if select_on == "test":
            sc.pp.highly_variable_genes(test_adata, n_top_genes=gene_no, subset=True)
        if select_on == "train":
            sc.pp.highly_variable_genes(train_adata, n_top_genes=gene_no, subset=True)

    if "FEAST" == select_method or "F-test" == select_method:
        ## read features selected by FEAST
        feast_file = result_dir+os.sep+select_method+"_features.txt"
        with open(feast_file) as f:
            feast_features = f.read().splitlines()
        feast_features = [x.upper() for x in feast_features] ## upper case

        if select_on == "test":
            feast_genes = set(feast_features).intersection(set(test_adata.var_names.tolist()))
            test_adata = test_adata[:, list(feast_genes)]
        if select_on == "train":
            feast_genes = set(feast_features).intersection(set(train_adata.var_names.tolist()))
            train_adata = train_adata[:, list(feast_genes)]

    features = set(train_adata.var_names.tolist()).intersection(set(test_adata.var_names.tolist()))
    features = list(features)
    features.sort()  ## for reproducibility
    print("Number of features:", len(features))

    ## write features into file
    with open(result_dir+os.sep+"features.txt", 'w') as f:
        for feature in features:
            f.write("%s\n" % feature)

    ## order common genes in anndata
    train_adata = train_adata[:, features]
    test_adata = test_adata[:, features]
    return train_adata, test_adata

def scale_and_visualize(train_adata, test_adata, result_dir, dr_seed=0, scale=True,
        plot=True, plot_elements=['dataset_batch', 'cell.type']):
    '''Scale data set and plot a dimension reduction on certain elements
    @dr_seed: seed for dimention reduction
    @plot_elements: dataset_batch, cell.type, or ind
    '''
    if scale:
        sc.pp.scale(train_adata, zero_center=True, max_value=6)
        sc.pp.scale(test_adata, zero_center=True, max_value=6)

    ## plot train_adata and test_adata
    adata = train_adata.concatenate(test_adata,join='inner',
            batch_key="dataset_batch",batch_categories=["train","test"]) #inner join

    if plot:
        if "ind" in adata.obs.columns:
            adata.obs["ind"] = adata.obs["ind"].astype("category")
        plot_adata(adata, plot_elements, result_dir, dr_seed=dr_seed)

    ## set adata information to train_adata, test_adata
    train_adata = adata[adata.obs[adata.obs["dataset_batch"] == "train"].index.tolist()]
    test_adata = adata[adata.obs[adata.obs["dataset_batch"] == "test"].index.tolist()]
    return train_adata, test_adata

def load_adata(result_dir):
    '''If data already exists, load from disk
    '''
    if (os.path.exists(result_dir+os.sep+"train_adata.h5ad") and
            os.path.exists(result_dir+os.sep+"test_adata.h5ad")):
        train_adata = anndata.read_h5ad(result_dir+os.sep+"train_adata.h5ad")
        test_adata = anndata.read_h5ad(result_dir+os.sep+"test_adata.h5ad")
        return True, train_adata, test_adata
    else:
        return False, None, None
 
def save_adata(train_adata, test_adata, result_dir, write=True):
    '''Save data to disk
    '''
    if write:
        train_adata.write(result_dir+os.sep+"train_adata.h5ad")
        test_adata.write(result_dir+os.sep+"test_adata.h5ad")

def plot_adata(adata, columns, result_dir, dr_seed=0, prefix=""):
    '''Dimension reduction on adata and plot out features of interest

    @ adata: combined anndata of train and test
    @ columns: columns of interest from adata.obs 
    @ result_dir: where to store the plots
    @ dr_seed: dimension reduction seed
    '''
    # do PCA first
    sc.tl.pca(adata, random_state=dr_seed)
    sc.tl.tsne(adata, use_rep="X_pca",
            learning_rate=300, perplexity=30, n_jobs=4, random_state=dr_seed)
    sc.pl.tsne(adata, color=columns)
    plt.savefig(result_dir+os.sep+prefix+"tSNE_cluster.png")

    ## do UMAP
    sc.pp.neighbors(adata, n_neighbors=20, use_rep="X_pca", random_state=dr_seed) 
    sc.tl.umap(adata, random_state=dr_seed)
    sc.pl.umap(adata, color=columns)
    plt.savefig(result_dir+os.sep+prefix+"umap_cluster.png")

def process_pipeline(train_adata, test_adata, result_dir, 
        gene_no=1000, select_on="test", select_method="Seurat",
        min_genes=10, min_cells=10):
    ''' A process pipeline integrating feature selection, center scaled the data
    '''
    ## feature selection
    train_adata, test_adata = feature_selection_train_test(train_adata, test_adata,
            result_dir, gene_no, select_on, select_method, min_genes, min_cells)

    ## if after feature selection, one of them is None, then train and test to None
    if train_adata is None or test_adata is None:
        return None, None

    ## scale and analyze
    train_adata, test_adata = scale_and_visualize(train_adata, test_adata, result_dir,
            plot=False)
    return train_adata, test_adata

def random_cv(adata, train_pct, sample_seed=0):
    '''Random split cross-validation train/test
    '''
    random.seed(sample_seed)
    train_cells = random.sample(adata.obs_names.tolist(), round(train_pct*adata.shape[0]))
    train_adata = adata[train_cells]
    test_cells = list(set(adata.obs_names) - set(train_cells))
    test_adata = adata[test_cells]
    return train_adata, test_adata


